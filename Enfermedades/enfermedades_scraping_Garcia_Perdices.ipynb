{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " download_file:  ['https://www.ecdc.europa.eu/sites/default/files/documents/AER_for_2017-Zika-virus-disease.pdf', 'https://www.ecdc.europa.eu/sites/default/files/documents/Zika-annual-epidemiological-report-2018.pdf', 'http://ecdc.europa.eu/sites/portal/files/documents/dengue-annual-epidemiological-report-2017.pdf', 'https://www.ecdc.europa.eu/sites/default/files/documents/dengue-annual-epidemiological-report-2018.pdf', 'http://ecdc.europa.eu/sites/portal/files/documents/chikungunya-virus-disease-annual-epidemiological-report-2017.pdf', 'https://www.ecdc.europa.eu/sites/default/files/documents/chikungunya-annual-epidemiological-report-2018.pdf']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "#Utilizaremos la url de búsqueda para buscar las enfermedades que realmente nos interesan\n",
    "#partimos de la url original que es : https://data.europa.eu/euodp/es/data/publisher/ecdc pero añadimos\n",
    "# \"?q=\"  para poder realizar la búsqueda \n",
    "url = 'https://data.europa.eu/euodp/es/data/publisher/ecdc?q='\n",
    "\n",
    "#variables que necesitamos:\n",
    "#enfermedades que queremos recuperar y estudiar\n",
    "enf_evaluar = ('Zika', 'Dengue', 'Chikungunya') \n",
    "\n",
    "#Años que vamos a evaluar\n",
    "year = ('2017', '2018')\n",
    "\n",
    "#lista donde guardaremos las urls de los datos\n",
    "download_file = []\n",
    "\n",
    "#Realizamos try/catch para detectar errores, realizamos catch de la excepción\n",
    "try:\n",
    "    \n",
    "    #Buscamos todas las enfermedades a estudiar\n",
    "    for i, enf in enumerate(enf_evaluar):\n",
    "        \n",
    "        #concatenamos la enfermedad a la url para ir directamente a la enfermedad buscada\n",
    "        broken_html = requests.get(url+enf_evaluar[i])\n",
    "        broken_html.raise_for_status()\n",
    "\n",
    "        # Pasamos el contenido HTML de la web a un objeto BeautifulSoup()\n",
    "        html = BeautifulSoup(broken_html.text, \"html.parser\")\n",
    "\n",
    "        # Obtenemos todos los li donde están las entradas\n",
    "        entradas = html.find_all('li', {'class': 'search-result-item'})\n",
    "\n",
    "        #recorremos los <li> con class: search-result-item  que es donde tenemos las enfermedades y sus links\n",
    "        for i, entrada in enumerate(entradas):\n",
    "\n",
    "            #Cogemos el texto para evaluar enfermedades. Con el método \"getText()\" no nos devuelve el HTML\n",
    "            enfermedad = entrada.find('a', {'class': 'item_link'}).find('strong').get_text()\n",
    "            #print('enfermedad: ', enfermedad)\n",
    "            \n",
    "            # REcuperamos el link que tenemos en el tag <a>\n",
    "            item_link = entrada.find('a', {'class': 'item_link'}).get('href')\n",
    "            #print('item_link: ', item_link)\n",
    "\n",
    "            #Para cada enfermedada que queremos\n",
    "            for i, enf in enumerate(enf_evaluar):\n",
    " \n",
    "                #Evaluamos si el texto contiene alguna de nuestras enfermedades\n",
    "                if (enfermedad.count(enf) > 0):\n",
    "\n",
    "                    #url de la enfermedad para ir a buscar las descargas: le pasamos el link obtenido anteriormente\n",
    "                    url_enf = item_link\n",
    "                    req_html = requests.get(url_enf)\n",
    "                    \n",
    "                    #página de la enfermedad\n",
    "                    enfermedad_html = BeautifulSoup(req_html.text, \"html.parser\")\n",
    "\n",
    "                    #recuperamos las entradas donde tenemos la lista de los link de descargas\n",
    "                    entradas_enf = enfermedad_html.find('ul', {'class': 'resource-list unstyled'}).find_all('span')\n",
    "\n",
    "                    #Recorremos la lista de links de descarga:\n",
    "                    for i, ent_enf in enumerate(entradas_enf):\n",
    "\n",
    "                        #Recuperamos el texto que acompaña a cada 'a' con class:name, es donde tenemos el texto\n",
    "                        descargas_text = ent_enf.find('a', {'class': 'name'})\n",
    "\n",
    "                   # como hemos recuperado todos los 'span', el que queremos 'a class:name' está dentro de un span determinado\n",
    "                   #por tanto, al recorrerlos todos no todos contendrán dicho tag, por tanto, al recuperar no tendremos nada\n",
    "                   #por no cumplir que contengan dicho tag.\n",
    "                        #Filtramos si tenemos tag 'a' con class: name\n",
    "                        if (descargas_text is not None): \n",
    "\n",
    "                            #recuperamos el texto de tag que tenemos: sí guardamos la 'descripción'\n",
    "                            descargas_text = descargas_text.get_text() \n",
    "\n",
    "                     #Cogemos solo los años que queremos evaluar, se ha comprobado que los años se encuentra en la descripción\n",
    "                     # y los ficheros que queremos evaluar que son los que contengan 'Epidemiological Report for' \n",
    "                            if((descargas_text.count(year[0]) > 0 or descargas_text.count(year[1]) > 0) \n",
    "                                              and descargas_text.count('Epidemiological Report for')):\n",
    " \n",
    "                                    #print('descargas_text :', descargas_text)\n",
    "    \n",
    "                                   # Obtenemos todos los links donde tendremos los datos\n",
    "                                   # para ello lo que hacemos es recuperar el tag anterior 'span' al que estamos trabajando.\n",
    "                                   # (en la web primero tenemos el link y después la descripción de ese link, al evaluar la\n",
    "                                   #descripción primero ya nos hemos pasado el link y por tanto lo recuperamos con previous)\n",
    "                                    span_link = ent_enf.find_previous_sibling('span')\n",
    "\n",
    "                                    #nos quedamos solo con la url\n",
    "                                    descargas_link = span_link.find('a').get('href')\n",
    "                                    #print('descarga link: ', descargas_link)\n",
    "                                    \n",
    "                                    if(descargas_link.count('pdf') > 0):\n",
    "                                        # Guardamos la url de referencia pdf\n",
    "                                        download_file.append(descargas_link)\n",
    "                                    \n",
    "                                    else:\n",
    "                                        #Leemos el link que nos lleva a donde tenemos el link de download\n",
    "                                        descarga = requests.get(descargas_link)\n",
    "                                          # Pasamos el contenido HTML de la web a un objeto BeautifulSoup()\n",
    "                                        descarga_html = BeautifulSoup(descarga.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                                        # Guardamos la url de referencia fichero\n",
    "                                        download_file.append(descarga_html.find('a', {'data-placement': \"bottom\"}).get('href'))\n",
    "                                   \n",
    "    print('\\n download_file: ', download_file)\n",
    "\n",
    "\n",
    "#else:\n",
    "#    print(\"error request: \", broken_html.status_code)\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print (\"Http Error:\",errh)\n",
    "except requests.exceptions.ConnectionError as errc:\n",
    "    print (\"Error Connecting:\",errc)\n",
    "except requests.exceptions.Timeout as errt:\n",
    "    print (\"Timeout Error:\",errt)\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print (\"OOps: Something Else\",err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#\n",
    "#           Ahora definimos una serie de funciones que vamos a utilizar\n",
    "#\n",
    "##################################################################################################\n",
    "\n",
    "# Función para Transformar las columnas\n",
    "def column_transform(table):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df = table \n",
    "    \n",
    "    #Renombramos las columnas\n",
    "    df.rename(columns=lambda x: x.replace('Unnamed: ', 'Class'), inplace=True)\n",
    "   \n",
    "    #columnas con nulos\n",
    "    null_columns=df.columns[df.isnull().any()]\n",
    "    sum_nan = df[null_columns].isnull().sum()       #contamos nulos de cada columna\n",
    "    nrow = df.shape[0]  #numero de filas\n",
    "\n",
    "    for i, j in enumerate(sum_nan):\n",
    "        if (j == nrow):\n",
    "            #eliminamos la columna con nulos en la segunda fila\n",
    "            col = null_columns[i]     # el nombre de  la columna que queremos eliminar\n",
    "            del df[col]\n",
    "\n",
    "    #miramos si tenemos el primer país de las tablas\n",
    "    if (df.iloc[0,0] != 'Austria'):\n",
    "        \n",
    "            df.iloc[0,:] = df.iloc[0,:].fillna('Rate')\n",
    "            #Ponemos la primera fila como encabezado del dataframe (la primera fila no son los datos)\n",
    "            df.columns = df.loc[0]\n",
    "\n",
    "            #Reorganizamos y eliminamos la primera fila que ahora la tenemos duplicada\n",
    "            df = df.reindex(df.index.drop(0))  \n",
    "            \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Función para renombrar las columnas iguales con el año que le toca\n",
    "def column_rename(table, year):\n",
    "    \n",
    "    #guardamos la tabla\n",
    "    df = table\n",
    "    \n",
    "    #df = column_transform(df)\n",
    "    \n",
    "    col_find = None\n",
    "    #Comprobamos si existe 'Confirmed' como columna. Evaluamos la columna transformada a string\n",
    "    for cols in df.columns.values:\n",
    "         if (str(cols).count('Confirmed') > 0): col_find = cols\n",
    "    \n",
    "    Switch = False\n",
    "    # Miramos si la búsqueda ha dado resultado            \n",
    "    if (not pd.isnull(col_find)): \n",
    "        fin = df.columns.get_loc(col_find) - 1     # si exsite le damos el valor que ocupa esta columna\n",
    "    else:  \n",
    "        fin = len(df.columns) - 1    # en caso contrario nos quedamos con el total de columnas - 1 (ya que python empieza en 0)\n",
    "        Switch = True\n",
    "    \n",
    "    #Cambiamos los nombres de las columnas repetidas para poder eliminarlas\n",
    "    #variables necesarias\n",
    "    a = year\n",
    "    year_array=[]\n",
    "    # El máximo rango para los años es 5 porque siempre tendremos 4 años atrás respecto al año que tratamos\n",
    "    if fin > 5:\n",
    "        max = 5\n",
    "    else: \n",
    "        max = fin\n",
    "    \n",
    "    for i in range(max):\n",
    "        a = year - ((max - 1) - i)       #el más pequeño primero      \n",
    "        year_array.append(str(a))   # lo convertimos en string para concatenar        \n",
    "    \n",
    "    # inicializamos variables\n",
    "    j = 0\n",
    "    cols = []\n",
    "    #A partir de la primera columna hasta la columna 'Confirmed' (o última columna) renombramos las columnas\n",
    "    #ya que tienen el mismo nombre y las distinguiremos por años\n",
    "    #  Vamos hacia atràs\n",
    "    for i,column in enumerate(df.columns):\n",
    "        if (i != 0 and i <= fin):\n",
    "            #si la columna no tiene valor nulo la renombramos añadiendo el año\n",
    "            cols.append(f'{column}_{year_array[j]}')\n",
    "            \n",
    "            #numeramos siempre que switch esté activado\n",
    "            if (Switch):\n",
    "                j = j + 1    \n",
    "            else:    \n",
    "                if (i%2 == 0):  j = j + 1\n",
    "            continue\n",
    "        cols.append(column)\n",
    "        \n",
    "    df.columns = cols\n",
    "    \n",
    "   \n",
    "    return df\n",
    "\n",
    "#Función para eliminar filas (filas primeras que no contienen paises)\n",
    "def row_delete(table):\n",
    "    \n",
    "    df = table\n",
    "    \n",
    "    #reseteamos índices\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    #borramos filas que no tengan el primer país\n",
    "    for i in range(4):\n",
    "        if (df.iloc[0,0] != 'Austria'):\n",
    "                #eliminamos  fila\n",
    "                 df = df.drop([i])\n",
    "                    \n",
    "    #reseteamos índices\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Función para dividir columnas que se hayan recuperados dobles\n",
    "def column_split(table, year):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #inicializamos variables\n",
    "    df = table\n",
    "    df2 = []\n",
    "    \n",
    "    #numero de columnas que tenemos\n",
    "    length_col = len(df.columns)\n",
    "    \n",
    "    #Empezamos en el cero y queremos la columna anterior a la última\n",
    "    col = length_col - 2\n",
    "    \n",
    "    #Si la columna anterior a la última se puede dividir en 2, es que tenemos 2 columnas dentro\n",
    "    if len(df.iloc[0, col].split(' ')) > 1:\n",
    "        #inicializamos variables\n",
    "        c1 = []\n",
    "        c2 = []\n",
    "        # Los nulos los ponemos como espacios (evitamos errores, pero los dejamos localizables)\n",
    "        df = df.fillna(' ')\n",
    "        \n",
    "        #Recorremos todas las filas de la columna doble y generamos dos columnas independientes\n",
    "        for i in range(df.shape[0]):\n",
    "            c1.append(str(df.iloc[i, col].split(' ')[0]))\n",
    "            c2.append(str(df.iloc[i, col].split(' ')[1]))\n",
    "        \n",
    "        #Creamos un nuevo dataframe para añadir las nuevas columnas:\n",
    "        #primero nos quedamos con las columnas anteriores a la duplicada\n",
    "        df2 = df.iloc[:,0:col]\n",
    "        \n",
    "        #añadimos c1 como columna siguiente (tendrá el mismo nombre columna doble)\n",
    "        df2[df.columns[col]] = c1\n",
    "        \n",
    "        #Añadimos c2 como columna siguiente (en este caso damos otro nombre, el nombres es sacado de los pdf's)\n",
    "        df2[f'ASR_{year}'] = c2\n",
    "        \n",
    "        #Añadimos la última columna del dataframe original\n",
    "        df2[df.columns[(length_col - 1)]] = df.iloc[:,-1]\n",
    "        \n",
    "        #devolvemos el dataframe resultante\n",
    "        df = df2\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url pdf: https://www.ecdc.europa.eu/sites/default/files/documents/AER_for_2017-Zika-virus-disease.pdf\n",
      "Guardado csv:  AER_for_2017-Zika-virus-disease.csv\n",
      "url pdf: https://www.ecdc.europa.eu/sites/default/files/documents/Zika-annual-epidemiological-report-2018.pdf\n",
      "Guardado csv:  Zika-annual-epidemiological-report-2018.csv\n",
      "url pdf: http://ecdc.europa.eu/sites/portal/files/documents/dengue-annual-epidemiological-report-2017.pdf\n",
      "Guardado csv:  dengue-annual-epidemiological-report-2017.csv\n",
      "url pdf: https://www.ecdc.europa.eu/sites/default/files/documents/dengue-annual-epidemiological-report-2018.pdf\n",
      "Guardado csv:  dengue-annual-epidemiological-report-2018.csv\n",
      "url pdf: http://ecdc.europa.eu/sites/portal/files/documents/chikungunya-virus-disease-annual-epidemiological-report-2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: abr 04, 2020 12:35:06 PM org.apache.pdfbox.pdmodel.font.PDCIDFontType2 <init>\r\n",
      "INFORMACIÓN: OpenType Layout tables used in font ABCDEE+Tahoma,Bold are not implemented in PDFBox and will be ignored\r\n",
      "abr 04, 2020 12:35:07 PM org.apache.pdfbox.pdmodel.font.PDCIDFontType2 <init>\r\n",
      "INFORMACIÓN: OpenType Layout tables used in font ABCDEE+Tahoma,Bold are not implemented in PDFBox and will be ignored\r\n",
      "abr 04, 2020 12:35:07 PM org.apache.pdfbox.pdmodel.font.PDCIDFontType2 <init>\r\n",
      "INFORMACIÓN: OpenType Layout tables used in font ABCDEE+Tahoma,Bold are not implemented in PDFBox and will be ignored\r\n",
      "abr 04, 2020 12:35:07 PM org.apache.pdfbox.pdmodel.font.PDCIDFontType2 <init>\r\n",
      "INFORMACIÓN: OpenType Layout tables used in font ABCDEE+Tahoma are not implemented in PDFBox and will be ignored\r\n",
      "abr 04, 2020 12:35:07 PM org.apache.pdfbox.pdmodel.font.PDCIDFontType2 <init>\r\n",
      "INFORMACIÓN: OpenType Layout tables used in font ABCDEE+Tahoma are not implemented in PDFBox and will be ignored\r\n",
      "abr 04, 2020 12:35:07 PM org.apache.pdfbox.pdmodel.font.PDCIDFontType2 <init>\r\n",
      "INFORMACIÓN: OpenType Layout tables used in font ABCDEE+Tahoma,Bold are not implemented in PDFBox and will be ignored\r\n",
      "abr 04, 2020 12:35:07 PM org.apache.pdfbox.pdmodel.font.PDCIDFontType2 <init>\r\n",
      "INFORMACIÓN: OpenType Layout tables used in font ABCDEE+Tahoma are not implemented in PDFBox and will be ignored\r\n",
      "abr 04, 2020 12:35:07 PM org.apache.pdfbox.pdmodel.font.PDCIDFontType2 <init>\r\n",
      "INFORMACIÓN: OpenType Layout tables used in font ABCDEE+Tahoma,Bold are not implemented in PDFBox and will be ignored\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado csv:  chikungunya-virus-disease-annual-epidemiological-report-2017.csv\n",
      "url pdf: https://www.ecdc.europa.eu/sites/default/files/documents/chikungunya-annual-epidemiological-report-2018.pdf\n",
      "Guardado csv:  chikungunya-annual-epidemiological-report-2018.csv\n",
      "\n",
      "\n",
      " Ya tienes todas tus tablas\n",
      "\n",
      " --\t DISFURTA  ---\n"
     ]
    }
   ],
   "source": [
    "#Hemos de instalar tabula-py si no lo tenemos instalado\n",
    "#!pip3 install tabula-py\n",
    "\n",
    "#Nos importamos la librería necesaria\n",
    "import tabula\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "    \n",
    "\n",
    "#!java -version ---> Nos hemos de asegurar que tenemos  Java instalado y en variables de entorno\n",
    "\n",
    "#En la página dos tenemos la tabla. \n",
    "#Lattice = True obliga a extraer los PDF mediante la extracción en modo reticular. \n",
    "#Reconoce cada una de las celdas en función de líneas de control o bordes de cada celda.\n",
    "#url consultada: \"https://aegis4048.github.io/parse-pdf-files-while-retaining-structure-with-tabula-py\"\n",
    "\n",
    "for i in range(len(download_file)):\n",
    "    pdf_file_path = download_file[i]\n",
    "    print('url pdf:', pdf_file_path)\n",
    "    df_result = []\n",
    "    table = []\n",
    "    \n",
    "    #Para Zika 2018 tenemos que seguir otro formato de extracción\n",
    "    if (pdf_file_path.count('Zika') > 0 and pdf_file_path.count('2018') > 0):    \n",
    "        table = tabula.read_pdf(pdf_file_path, lattice=True,pages = \"2,3\", guess = False)\n",
    "    else:\n",
    "        table = tabula.read_pdf(pdf_file_path, pages = \"2,3\", multiple_tables=True)\n",
    "\n",
    "    if (pdf_file_path.count('2017') > 0): \n",
    "        year = 2017\n",
    "    else: \n",
    "        if(pdf_file_path.count('2018') > 0): \n",
    "            year = 2018\n",
    "\n",
    "    #La tabla es una lista de 2 elementos. En uno de los dos elementos tenemos la tabla\n",
    "    #print(table)\n",
    "    if (type(table) is list):\n",
    "        for j in range(len(table)):\n",
    "            if (isinstance(table[j], pd.DataFrame) and len(table[j].columns) > 3):\n",
    "                df_result=table[j]\n",
    "                df_result=column_transform(df_result)\n",
    "\n",
    "                df_result=column_rename(df_result,year)\n",
    "\n",
    "                df_result=row_delete(df_result)\n",
    "                \n",
    "                df_result=column_split(df_result,year)\n",
    "\n",
    "    #renombramos primera columna\n",
    "    df_result = df_result.rename(columns={df_result.columns[0]: 'Country'}) \n",
    "    \n",
    "    filename = pdf_file_path.split('/')[7].split('.')[0] + '.csv'\n",
    "    df_result.to_csv(filename,  sep='\\t', encoding='utf-8')\n",
    "    print('Guardado csv: ', filename)\n",
    "\n",
    "print('\\n\\n Ya tienes todas tus tablas\\n\\n --\\t DISFURTA  ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
